from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import pickle
import sys
import get_data_and_labels_by_class as gdl
import load_parameters as lp
import greedy_pruning as gp

'''
1) Load original network parameters
2) Load the sequence of filters that need to be removed (note: one can modify what filters are to be removed, 
as long as the sequence is a list of lists of the kind:
    [ [list of filters to remove from 1st layer], [list of filters to remove from 2nd layer],  ...]
3) Get the i-th branch which has the filters removed according to the i-th element of the sequence of filters that need to be removed
4) Feed-forward the data, grouped by class, through the i-th branch 
5) Record accuracy per class from branch i
'''

########################################################################################################################
# 1) Load the original network parameters

path_to_weights = 'parameters_original_net/parameters_epoch_10'
path_to_sequence_of_filters_to_be_removed = 'filters_to_remove'
branch_index = int(sys.argv[1])
num_layers = 5 #this should include the fully connected layer in the count
num_conv_layers = 4
epoch = 10
total_number_of_filters = 0
number_of_filters_per_layer = []


w = lp.load_weights(path_to_weights, num_layers, epoch)
b = lp.load_biases(path_to_weights, num_layers, epoch)

#get the total number of filters and number of filters per layer, as we need then as input to the get_branch function
for i in range(num_conv_layers):
    total_number_of_filters+=w[i][0].shape[0]
    number_of_filters_per_layer.append(w[i][0].shape[0])

########################################################################################################################
# 2) Load the sequence of filters to be removed; those were generated by generate_sequence_of_filters_to_be_removed.py

sequence_of_filters_to_be_removed = pickle.load(open(path_to_sequence_of_filters_to_be_removed, 'rb'))



########################################################################################################################
# 3) Get the i-th branch of the network, according to the i-th sequence to be removed

branch = branch_index
layer_to_remove_filter_from = 0
for i in range(num_conv_layers):
    branch -= number_of_filters_per_layer[i]
    if branch < 0:
        layer_to_remove_filter_from = i
        break

print("We are removing filter ", sequence_of_filters_to_be_removed[branch_index][layer_to_remove_filter_from], " from layer ", layer_to_remove_filter_from)
w_new, b_new = gp.create_a_branch_from_original_net(w, b, epoch=len(w[0])-1, layer_to_remove_filter_from=layer_to_remove_filter_from, index_of_filter_to_remove=sequence_of_filters_to_be_removed[branch_index][layer_to_remove_filter_from])


########################################################################################################################
# 4) Feed-forward the data, grouped by class, through the i-th branch


num_fully_connected = w_new[num_conv_layers-1][0].shape[0]
num_classes = 10
accuracy_per_class_for_branch = []

class Net(nn.Module):

    def __init__(self, weights, biases, num_classes):
        total_num_epochs_considered = len(w_new[0])-1
        super(Net, self).__init__()

        self.conv1 = nn.Conv2d(1, weights[0][total_num_epochs_considered].shape[0], kernel_size=9)
        self.conv2 = nn.Conv2d(weights[1][total_num_epochs_considered].shape[1], weights[1][total_num_epochs_considered].shape[0], kernel_size=9)
        self.conv3 = nn.Conv2d(weights[2][total_num_epochs_considered].shape[1], weights[2][total_num_epochs_considered].shape[0], kernel_size=8)
        self.conv4 = nn.Conv2d(weights[3][total_num_epochs_considered].shape[1], weights[3][total_num_epochs_considered].shape[0], kernel_size=1)
        self.fc1 = nn.Linear(weights[3][total_num_epochs_considered].shape[0]*5*5, num_classes)
        #self.lsm = nn.LogSoftmax()


        self.conv1.weight.data = weights[0][total_num_epochs_considered]
        self.conv2.weight.data = weights[1][total_num_epochs_considered]
        self.conv3.weight.data = weights[2][total_num_epochs_considered]
        self.conv4.weight.data = weights[3][total_num_epochs_considered]
        self.fc1.weight.data = weights[4][total_num_epochs_considered]


        self.conv1.bias.data = biases[0][total_num_epochs_considered]
        self.conv2.bias.data = biases[1][total_num_epochs_considered]
        self.conv3.bias.data = biases[2][total_num_epochs_considered]
        self.conv4.bias.data = biases[3][total_num_epochs_considered]
        self.fc1.bias.data = biases[4][total_num_epochs_considered]


    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.conv3(x)
        x = F.relu(x)
        x = self.conv4(x)
        x = F.relu(x)
        x = x.view(-1, num_fully_connected * 5 * 5)
        x = self.fc1(x)

        return F.log_softmax(x, dim=1)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

# Training settings
batch_size = 800

#data loading
train_dataset = datasets.MNIST(root='./data/',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='./data/',
                              train=False,
                              transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)


labels = gdl.get_all_labels(test_loader)
data = gdl.get_all_data(test_loader)

def test(data, labels, considered_class, num_classes):
    model.eval()
    test_loss = 0
    correct = 0
    labels = gdl.get_all_labels(test_loader)
    data = gdl.get_all_data(test_loader)
    groups = gdl.get_groups_of_class_indices(labels, num_classes)
    data_to_consider = data[tuple(groups[considered_class])]
    labels_to_consider = torch.tensor(labels[tuple(groups[considered_class])])
    output = model(torch.tensor(data_to_consider))

    # sum up batch loss
    test_loss += F.nll_loss(output, labels_to_consider).data.item()
    # get the index of the max log-probability
    pred = output.data.max(1, keepdim=True)[1]
    correct += pred.eq(labels_to_consider.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Accuracy: {}/{} ({:.0f}%)\n'.format(
            correct, len(labels_to_consider.data),
        100. * correct / len(labels_to_consider.data)))
    accuracy_per_class_for_branch.append(correct)

model = Net(w_new, b_new, num_classes=num_classes)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

for considered_class in range(num_classes):
    print(considered_class)
    test(data=data, labels=labels, considered_class=considered_class, num_classes=num_classes)


########################################################################################################################
# 5) Finally, record the accuracy per class from this branch

fw = open('accuracies_from_branch/accuracies_branch'+str(branch_index), 'wb')
pickle.dump(accuracy_per_class_for_branch, fw)
fw.close()